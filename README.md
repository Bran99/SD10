# SD10


**Design a web crawler that will systematically browse and download world wide web.**

**What to be designed?**

1. A web crawler is a software program that browses world wide web in a methodical and automated manner.It collects documents by recursively fetching links from a set of starting pages.
2. Web crawlers are used by search engines to provide up-to-date data. Search engines download all pages to create an index on them to perform faster searches.
3. Other use cases include
    a. Test web pages and links for valid structure.
    b. Monitor sites to see where structure or content change.
    c. Maintain mirror sited for popular web sites.
    d. Seach copyright infrigements.
    e. Build special purpose index. (to understand content stored in multimedia files on the web)


**TO DO : Follow the below framweork to design System. Mandatory upload of hand drawn photos of design.**

**Capacity Estimation and Constraints**

What will your estimations of scale? As a system design student you should be able to make intelligent assumptions based on real life systems. 

**System APIs**

**Database Design**

**Basic System Design and Algorithm**

**Data Partitioning and Replication**

**Caching**

**Load Balancing**

**Handling Edge cases in given system**